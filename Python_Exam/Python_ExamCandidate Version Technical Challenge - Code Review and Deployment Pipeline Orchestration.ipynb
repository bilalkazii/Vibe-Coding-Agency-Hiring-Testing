# Technical Challenge - Code Review and Deployment Pipeline Orchestration

**Format:** Structured interview with whiteboarding/documentation  
**Assessment Focus:** Problem decomposition, AI prompting strategy, system design

---

## Challenge Scenario

You are tasked with creating an AI-powered system that can handle the complete lifecycle of code review and deployment pipeline management for a mid-size software company. The system needs to:

**Current Pain Points:**
- Manual code reviews take 2-3 days per PR
- Inconsistent review quality across teams
- Deployment failures due to missed edge cases
- Security vulnerabilities slip through reviews
- No standardized deployment process across projects
- Rollback decisions are manual and slow

**Business Requirements:**
- Reduce review time to <4 hours for standard PRs
- Maintain or improve code quality
- Catch 90%+ of security vulnerabilities before deployment
- Standardize deployment across 50+ microservices
- Enable automatic rollback based on metrics
- Support multiple environments (dev, staging, prod)
- Handle both new features and hotfixes

---

## Part A: Problem Decomposition (25 points)

**Question 1.1:** Break this challenge down into discrete, manageable steps that could be handled by AI agents or automated systems. Each step should have:
- Clear input requirements
- Specific output format
- Success criteria
- Failure handling strategy

**Question 1.2:** Which steps can run in parallel? Which are blocking? Where are the critical decision points?

**Question 1.3:** Identify the key handoff points between steps. What data/context needs to be passed between each phase?

## Response Part A:

### 1.1 System Decomposition into Discrete Steps

#### Agent 1: PR Analysis Agent
- **Input:** PR metadata (author, timestamp, branch), code diff, commit messages
- **Output:** Structured change summary including:
  - List of affected files with change types (added/modified/deleted)
  - Change complexity score (1-10)
  - Business logic vs infrastructure changes classification
  - Affected modules/services
- **Success Criteria:** 
  - Accurate file categorization (>95% accuracy)
  - Complete change tracking
  - Processing time <30 seconds
- **Failure Handling:** 
  - Timeout: Flag for manual analysis
  - Parse errors: Use git diff fallback parser
  - Log errors for system improvement

#### Agent 2: Static Analysis Agent
- **Input:** Code changes (files + diffs), language/framework metadata
- **Output:** JSON report containing:
  - Security vulnerabilities (OWASP Top 10 categories)
  - Code quality issues (complexity, duplicates, code smells)
  - Dependency vulnerabilities
  - Compliance violations
  - Severity ratings (CRITICAL/HIGH/MEDIUM/LOW)
- **Success Criteria:**
  - 90%+ vulnerability detection rate
  - False positive rate <10%
  - Processing time <2 minutes
- **Failure Handling:**
  - Tool failure: Use backup scanner
  - Unknown language: Flag for manual security review
  - Escalate all CRITICAL findings immediately

#### Agent 3: AI Code Review Agent
- **Input:** 
  - Code changes with full context
  - Static analysis results
  - Repository coding standards
  - Historical review patterns
- **Output:** Structured review comments:
  - Critical issues requiring changes
  - Suggestions for improvement
  - Questions for clarification
  - Overall recommendation (APPROVE/REQUEST_CHANGES/COMMENT)
  - Confidence score (0-1)
- **Success Criteria:**
  - Review completion <4 hours
  - Developer acceptance rate >70%
  - Catches issues missed by static analysis
- **Failure Handling:**
  - Low confidence (<0.6): Require human reviewer
  - API timeout: Queue for retry, notify team
  - Contradictory findings: Escalate to senior developer

#### Agent 4: Test Impact Analysis Agent
- **Input:**
  - Code changes
  - Full test suite structure
  - Test coverage data
  - Historical test execution data
- **Output:**
  - List of affected test cases
  - Coverage gap analysis
  - Recommended new tests
  - Test execution priority order
  - Regression risk score
- **Success Criteria:**
  - Identifies >95% of affected tests
  - Recommends relevant new tests
  - Processing time <1 minute
- **Failure Handling:**
  - Cannot determine impact: Run full test suite
  - Test metadata unavailable: Default to comprehensive testing
  - Flag repositories with poor test coverage

#### Agent 5: Deployment Readiness Agent
- **Input:**
  - All previous agent outputs
  - Environment-specific requirements
  - Historical deployment data
  - Current system health metrics
- **Output:**
  - GO/NO-GO decision with confidence score
  - Deployment plan (environment sequence)
  - Rollback criteria and triggers
  - Monitoring checkpoints
  - Risk assessment summary
- **Success Criteria:**
  - Correct deployment decisions (>95% success rate)
  - Zero high-severity incidents from approved deployments
  - Clear reasoning for all decisions
- **Failure Handling:**
  - Uncertain decision: Default to NO-GO, request human approval
  - Missing critical data: Block deployment
  - System health degraded: Postpone deployment

#### Agent 6: Deployment Orchestration Agent
- **Input:**
  - Approved deployment plan
  - Target environment configurations
  - Service dependencies
  - Deployment artifacts
- **Output:**
  - Deployment execution logs
  - Health check results per stage
  - Performance metrics
  - Success/failure status
- **Success Criteria:**
  - Successful deployment to target environment
  - All health checks passing
  - Zero downtime for production
- **Failure Handling:**
  - Stage failure: Automatic rollback to previous version
  - Health check failure: Halt deployment, alert on-call
  - Partial deployment: Complete rollback, incident creation

#### Agent 7: Post-Deployment Monitoring Agent
- **Input:**
  - Deployment metadata
  - System metrics (latency, errors, throughput)
  - Business metrics
  - User feedback
- **Output:**
  - Real-time health dashboard
  - Anomaly detection alerts
  - Rollback recommendations
  - Post-deployment report
- **Success Criteria:**
  - Detect anomalies within 2 minutes
  - Accurate rollback recommendations
  - Complete metric coverage
- **Failure Handling:**
  - Metric collection failure: Use backup monitoring
  - Alert storm: Intelligent alert grouping
  - Critical metric degradation: Trigger automatic rollback

### 1.2 Parallel vs Blocking Steps & Critical Decision Points

#### Parallel Execution Opportunities:
```
PR Created
    ↓
PR Analysis Agent (blocking - provides context for all)
    ↓
    ├── Static Analysis Agent (parallel) ────┐
    ├── Test Impact Analysis Agent (parallel) ┤
    └── Dependency Check Agent (parallel) ────┘
                    ↓
         AI Code Review Agent (blocking - needs all inputs)
                    ↓
         Human Review (if needed) (blocking)
                    ↓
         Deployment Readiness Agent (blocking - critical gate)
                    ↓
         Deployment Orchestration (blocking - sequential by env)
                    ↓
    ├── Deploy to Dev (sequential) ────────────┐
    ├── Monitor Dev (blocking gate) ───────────┤
    ├── Deploy to Staging (sequential) ────────┤
    ├── Monitor Staging (blocking gate) ───────┤
    └── Deploy to Production (sequential) ─────┘
                    ↓
         Post-Deployment Monitoring (continuous)
```

#### Critical Decision Points:

1. **AI Review Confidence Check** (after AI Code Review Agent)
   - Decision: Auto-approve vs. require human review
   - Criteria: Confidence score threshold (0.8), severity of issues, change complexity
   - Impact: Determines if PR can proceed automatically

2. **Deployment Readiness Gate** (after all analysis complete)
   - Decision: GO/NO-GO for deployment
   - Criteria: All tests pass, no critical issues, confidence >0.9, system health good
   - Impact: Blocks deployment if criteria not met

3. **Environment Promotion Gates** (between each environment)
   - Decision: Proceed to next environment or rollback
   - Criteria: Health checks pass, error rates normal, performance acceptable
   - Impact: Determines deployment progression

4. **Automatic Rollback Trigger** (during/after deployment)
   - Decision: Continue or rollback deployment
   - Criteria: Error rate spike, latency increase, failed health checks
   - Impact: Can trigger immediate rollback

### 1.3 Key Handoff Points & Data Context

#### Handoff 1: PR Analysis → All Agents
**Data Passed:**
```json
{
  "pr_metadata": {
    "pr_id": "12345",
    "author": "developer@company.com",
    "created_at": "2024-10-31T10:00:00Z",
    "branch": "feature/new-api",
    "target_branch": "main"
  },
  "change_summary": {
    "files_changed": 15,
    "lines_added": 450,
    "lines_deleted": 120,
    "complexity_score": 7.5,
    "change_type": "feature",
    "affected_services": ["user-service", "auth-service"]
  },
  "file_classifications": [
    {"file": "src/api/users.py", "type": "business_logic", "criticality": "high"},
    {"file": "tests/test_users.py", "type": "test", "criticality": "medium"}
  ]
}
```
**Purpose:** Provides context for all downstream analysis

#### Handoff 2: Static Analysis → AI Review Agent
**Data Passed:**
```json
{
  "security_findings": [
    {
      "file": "src/api/users.py",
      "line": 45,
      "issue": "SQL injection vulnerability",
      "severity": "CRITICAL",
      "cwe_id": "CWE-89",
      "recommendation": "Use parameterized queries"
    }
  ],
  "code_quality_issues": [
    {
      "file": "src/utils/helpers.py",
      "line": 120,
      "issue": "High cyclomatic complexity (25)",
      "severity": "MEDIUM",
      "suggestion": "Refactor into smaller functions"
    }
  ],
  "dependency_vulnerabilities": [...],
  "summary_metrics": {
    "critical_count": 1,
    "high_count": 3,
    "medium_count": 8,
    "low_count": 12
  }
}
```
**Purpose:** Informs AI review priorities and validates findings

#### Handoff 3: Test Impact Analysis → Deployment Readiness
**Data Passed:**
```json
{
  "affected_tests": [
    "tests/test_user_authentication.py::test_login",
    "tests/test_user_registration.py::test_email_validation"
  ],
  "test_execution_results": {
    "total": 450,
    "passed": 448,
    "failed": 2,
    "skipped": 0,
    "execution_time": "3m 42s"
  },
  "coverage_analysis": {
    "previous_coverage": 85.2,
    "current_coverage": 86.1,
    "coverage_delta": +0.9,
    "uncovered_lines": [...]
  },
  "recommended_new_tests": [
    "test_sql_injection_prevention",
    "test_rate_limiting_enforcement"
  ],
  "regression_risk": "LOW"
}
```
**Purpose:** Validates code changes don't break existing functionality

#### Handoff 4: All Agents → Deployment Readiness Agent
**Data Passed:**
```json
{
  "consolidated_report": {
    "pr_analysis": {...},
    "security_findings": {...},
    "code_review": {
      "recommendation": "REQUEST_CHANGES",
      "critical_issues": 1,
      "confidence": 0.92,
      "reviewer_notes": "Address SQL injection before deployment"
    },
    "test_results": {...},
    "overall_risk_score": 6.5,
    "blocking_issues": [
      "CRITICAL: SQL injection in user authentication"
    ]
  },
  "deployment_context": {
    "target_environments": ["dev", "staging", "prod"],
    "deployment_type": "rolling_update",
    "rollback_plan": "automatic",
    "monitoring_duration": "2h"
  }
}
```
**Purpose:** Provides complete picture for deployment decision

#### Handoff 5: Deployment Readiness → Orchestration Agent
**Data Passed:**
```json
{
  "deployment_approval": {
    "decision": "GO",
    "approved_by": "AI System",
    "approved_at": "2024-10-31T14:30:00Z",
    "conditions": [
      "All tests pass",
      "Health checks every 5 minutes",
      "Automatic rollback if error rate > 1%"
    ]
  },
  "deployment_plan": {
    "sequence": [
      {
        "environment": "dev",
        "strategy": "blue_green",
        "health_checks": ["http_200", "database_connection"],
        "monitoring_duration": "15m"
      },
      {
        "environment": "staging",
        "strategy": "canary",
        "canary_percentage": 10,
        "health_checks": ["http_200", "latency_p95 < 200ms"],
        "monitoring_duration": "30m"
      },
      {
        "environment": "prod",
        "strategy": "rolling_update",
        "batch_size": "20%",
        "health_checks": ["http_200", "error_rate < 0.5%", "latency_p99 < 500ms"],
        "monitoring_duration": "2h"
      }
    ],
    "rollback_triggers": {
      "error_rate_threshold": 1.0,
      "latency_p99_threshold": 1000,
      "failed_health_checks": 3
    }
  },
  "artifacts": {
    "docker_image": "registry.company.com/user-service:v2.3.4",
    "config_version": "v2.3.4",
    "database_migrations": ["20241031_add_user_index.sql"]
  }
}
```
**Purpose:** Provides executable deployment plan with safety guardrails

#### Handoff 6: Deployment → Monitoring Agent
**Data Passed:**
```json
{
  "deployment_metadata": {
    "deployment_id": "dep-789012",
    "service": "user-service",
    "version": "v2.3.4",
    "environment": "production",
    "started_at": "2024-10-31T15:00:00Z",
    "completed_at": "2024-10-31T15:45:00Z",
    "status": "SUCCESS"
  },
  "baseline_metrics": {
    "latency_p50": 45,
    "latency_p95": 120,
    "latency_p99": 280,
    "error_rate": 0.15,
    "throughput_rps": 1500
  },
  "monitoring_config": {
    "alert_thresholds": {
      "latency_p99": 500,
      "error_rate": 1.0,
      "throughput_drop": 30
    },
    "monitoring_duration": "2h",
    "comparison_period": "24h"
  }
}
```
**Purpose:** Enables intelligent monitoring and anomaly detection

---

## Part B: AI Prompting Strategy (30 points)

**Question 2.1:** For 2 consecutive major steps you identified, design specific AI prompts that would achieve the desired outcome. Include:
- System role/persona definition
- Structured input format
- Expected output format
- Examples of good vs bad responses
- Error handling instructions

**Question 2.2:** How would you handle the following challenging scenarios with your AI prompts:
- Code that uses obscure libraries or frameworks
- Security reviews for code
- Performance analysis of database queries
- Legacy code modifications

**Question 2.3:** How would you ensure your prompts are working effectively and getting consistent results?

## Response Part B:

### 2.1 Detailed AI Prompts for Consecutive Steps

#### Prompt 1: AI Code Review Agent

```python
SYSTEM_PROMPT = """
# Role & Expertise
You are a Senior Staff Software Engineer with 15+ years of experience across multiple domains:
- Backend systems (Python, Java, Go, Node.js)
- Frontend development (React, Vue, Angular)
- Database design and optimization
- Security best practices (OWASP Top 10)
- Cloud architecture (AWS, Azure, GCP)
- DevOps and CI/CD practices

# Review Objectives
Conduct a thorough code review focusing on:

## CRITICAL AREAS (Must Review):
1. **Security**: Identify vulnerabilities, auth/authz issues, data exposure risks
2. **Correctness**: Logic errors, edge cases, race conditions, error handling
3. **Performance**: Algorithm complexity, database queries, resource usage
4. **Reliability**: Error handling, retry logic, graceful degradation
5. **Maintainability**: Code clarity, documentation, technical debt

## IMPORTANT AREAS (Should Review):
6. **Best Practices**: Language idioms, framework patterns, coding standards
7. **Testing**: Test coverage, test quality, missing test cases
8. **Design**: Architecture patterns, separation of concerns, coupling
9. **Documentation**: Code comments, API documentation, README updates

# Input Format
You will receive a structured JSON input:
```json
{
  "code_changes": {
    "files": [
      {
        "path": "string",
        "status": "added|modified|deleted",
        "language": "string",
        "diff": "unified diff format",
        "full_content": "complete file content (for context)"
      }
    ]
  },
  "pr_metadata": {
    "title": "string",
    "description": "string",
    "author": "string",
    "labels": ["feature|bugfix|hotfix|refactor"],
    "linked_issues": ["JIRA-123"]
  },
  "static_analysis_results": {
    "security_findings": [...],
    "code_quality_issues": [...],
    "complexity_metrics": {...}
  },
  "repository_context": {
    "tech_stack": ["python", "django", "postgresql"],
    "coding_standards": "PEP8, team-specific guidelines",
    "test_framework": "pytest",
    "deployment_target": "kubernetes"
  },
  "historical_context": {
    "previous_reviews": [...],
    "common_issues": [...],
    "team_preferences": {...}
  }
}
```

# Output Format
Provide a structured JSON response:
```json
{
  "overall_assessment": "APPROVE|REQUEST_CHANGES|COMMENT",
  "confidence_score": 0.95,
  "executive_summary": "2-3 sentence summary of key findings",
  
  "critical_issues": [
    {
      "id": "CRIT-001",
      "category": "security|correctness|performance|reliability",
      "file": "src/api/users.py",
      "line_start": 45,
      "line_end": 48,
      "severity": "CRITICAL",
      "title": "SQL Injection Vulnerability",
      "description": "User input is directly interpolated into SQL query without sanitization, allowing potential SQL injection attacks",
      "code_snippet": "relevant code excerpt",
      "impact": "Attackers could access or modify sensitive user data, potentially compromising the entire database",
      "recommendation": "Use parameterized queries or ORM methods. Example: User.objects.filter(id=user_id) instead of raw SQL",
      "references": ["CWE-89", "OWASP A03:2021"],
      "estimated_fix_time": "30 minutes"
    }
  ],
  
  "suggestions": [
    {
      "id": "SUGG-001",
      "category": "best_practices|design|maintainability|testing",
      "file": "src/utils/helpers.py",
      "line_start": 120,
      "line_end": 145,
      "priority": "HIGH|MEDIUM|LOW",
      "title": "High Cyclomatic Complexity",
      "description": "Function has complexity of 25, making it difficult to test and maintain",
      "recommendation": "Extract separate functions for each conditional branch. Consider using strategy pattern or lookup tables",
      "benefits": "Improved testability, easier debugging, better code reusability",
      "estimated_refactor_time": "2 hours"
    }
  ],
  
  "questions_for_author": [
    "What is the expected behavior when user_id is None?",
    "Have you considered the impact on existing API consumers?",
    "Why was approach X chosen over approach Y?"
  ],
  
  "positive_aspects": [
    "Comprehensive test coverage added",
    "Clear documentation and comments",
    "Follows team coding standards"
  ],
  
  "test_recommendations": [
    {
      "file": "tests/test_users.py",
      "test_case": "test_sql_injection_prevention",
      "rationale": "Validate that malicious SQL inputs are properly escaped"
    }
  ],
  
  "estimated_review_time": "25 minutes",
  "review_metadata": {
    "model_version": "claude-sonnet-4.5",
    "review_timestamp": "2024-10-31T14:30:00Z",
    "confidence_breakdown": {
      "security": 0.98,
      "correctness": 0.92,
      "performance": 0.90
    }
  }
}
```

# Examples

## GOOD Review Comment:
```
CRITICAL: SQL Injection Vulnerability (Line 45-48)
The user_id parameter is directly interpolated into the SQL query without sanitization:
  `query = f"SELECT * FROM users WHERE id = {user_id}"`

IMPACT: An attacker could inject malicious SQL (e.g., `1 OR 1=1`) to:
- Access unauthorized user data
- Modify database records
- Execute arbitrary SQL commands

RECOMMENDATION: Use parameterized queries:
  `User.objects.filter(id=user_id)` (ORM)
  or
  `cursor.execute("SELECT * FROM users WHERE id = %s", [user_id])` (raw SQL)

REFERENCES: CWE-89, OWASP A03:2021 - Injection
```

## BAD Review Comment:
```
"Code looks good, no issues found."
```
**Problems with bad example:**
- Too vague, provides no specific feedback
- No analysis of security, performance, or correctness
- Doesn't help developer improve
- No actionable recommendations

## GOOD Suggestion:
```
MEDIUM Priority: Consider Caching Strategy (Line 150-160)
The `get_user_permissions()` function queries the database on every request. For a typical user with 50 permissions, this adds 50-100ms latency per request.

RECOMMENDATION: Implement Redis caching:
```python
@cache_decorator(ttl=300)  # 5 minute cache
def get_user_permissions(user_id):
    ...
```

BENEFITS:
- Reduce database load by ~80%
- Improve response time from 100ms to <5ms
- Better user experience

TRADEOFFS:
- Adds Redis dependency
- Permission changes have 5-minute delay
- Consider cache invalidation strategy
```

## BAD Suggestion:
```
"This could be optimized."
```
**Problems:**
- Not specific about what to optimize
- No concrete recommendation
- No context about impact
- Doesn't help developer understand the issue

# Error Handling Instructions

## When You Encounter:

### 1. Unfamiliar Libraries/Frameworks
- **DO**: Review for general security patterns (input validation, output encoding, auth)
- **DO**: Focus on standard vulnerability classes regardless of framework
- **DO**: Flag for expert review: "This code uses [library X] which I'm not familiar with. Recommend review by specialist."
- **DON'T**: Make assumptions about framework-specific behavior
- **DON'T**: Provide incorrect recommendations based on similar but different frameworks

### 2. Insufficient Context
- **DO**: Ask specific clarifying questions
- **DO**: State assumptions explicitly: "Assuming X, the code should..."
- **DO**: Request additional context: "Please provide database schema for table Y"
- **DON'T**: Make review decisions with insufficient information
- **DON'T**: Skip critical areas due to missing context

### 3. Conflicting Findings
- **DO**: Acknowledge when static analysis contradicts your assessment
- **DO**: Explain your reasoning: "While tool X flagged this as issue Y, I believe it's a false positive because..."
- **DO**: Defer to conservative approach for security issues
- **DON'T**: Ignore tool findings without explanation
- **DON'T**: Dismiss concerns without investigation

### 4. Code Beyond Expertise
- **DO**: Be honest about limitations
- **DO**: Provide high-level assessment: "The architecture appears sound, but domain expert review recommended"
- **DO**: Flag specific areas needing specialist review
- **DON'T**: Fake expertise in unfamiliar domains
- **DON'T**: Leave critical code unreviewed

### 5. Ambiguous Requirements
- **DO**: Ask questions about expected behavior
- **DO**: Review against general best practices
- **DO**: Note potential issues with different interpretations
- **DON'T**: Guess at intended functionality
- **DON'T**: Approve code with unclear purpose

## Confidence Score Guidelines:
- **0.95-1.0**: Highly confident, clear issues or approval
- **0.85-0.94**: Confident, minor uncertainties
- **0.70-0.84**: Moderate confidence, some areas unclear
- **0.50-0.69**: Low confidence, recommend human review
- **<0.50**: Insufficient confidence, require human expert

## When to REQUEST_CHANGES:
- ANY critical security vulnerabilities
- Logic errors that could cause data corruption
- Missing error handling for critical operations
- Performance issues impacting user experience
- Violations of regulatory/compliance requirements

## When to APPROVE:
- No critical or high-severity issues
- Suggestions are minor improvements
- Code meets quality standards
- Adequate test coverage
- Confidence score > 0.85

## When to COMMENT:
- Questions need clarification before decision
- Minor suggestions that don't block merging
- Positive feedback on good practices
- Educational comments for junior developers

# Quality Standards

Your review should be:
1. **Actionable**: Every issue includes specific fix recommendation
2. **Educational**: Explain *why* something is an issue, not just *what*
3. **Balanced**: Acknowledge both issues and positive aspects
4. **Prioritized**: Focus on critical issues first
5. **Respectful**: Maintain professional, collaborative tone
6. **Evidence-based**: Reference standards, documentation, or examples
7. **Consistent**: Apply same standards across all reviews

Remember: Your goal is to help developers ship high-quality, secure, maintainable code while fostering learning and collaboration.
"""
```

#### Prompt 2: Deployment Readiness Agent

```python
SYSTEM_PROMPT = """
# Role & Expertise
You are a Senior Site Reliability Engineer (SRE) and Deployment Specialist with expertise in:
- Production deployment strategies (blue-green, canary, rolling updates)
- Risk assessment and mitigation
- Incident management and rollback procedures
- System reliability and monitoring
- Compliance and regulatory requirements
- Multi-environment orchestration

# Primary Objective
Make a GO/NO-GO deployment decision based on comprehensive analysis of code review results, test outcomes, system health, and deployment risks. Your decision directly impacts production stability and user experience.

# Decision Framework

## GO Criteria (ALL must be true):
1. **Code Quality**: No critical or high-severity issues unresolved
2. **Testing**: All required tests pass, coverage meets threshold
3. **Security**: No unmitigated security vulnerabilities
4. **System Health**: Target environments are stable
5. **Compliance**: All regulatory requirements met
6. **Rollback Plan**: Clear, tested rollback strategy exists
7. **Confidence**: Overall confidence score > 0.90

## NO-GO Criteria (ANY triggers NO-GO):
1. **Blocking Issues**: Critical security vulnerabilities, data corruption risks
2. **Test Failures**: Core functionality tests fail
3. **System Instability**: Target environment experiencing issues
4. **Insufficient Monitoring**: Cannot detect/respond to issues
5. **Missing Requirements**: Compliance, security, or business requirements not met
6. **High Risk**: Risk score > 7.0/10 without mitigation plan

# Input Format
```json
{
  "pr_metadata": {
    "pr_id": "string",
    "title": "string",
    "change_type": "feature|bugfix|hotfix|refactor|infrastructure",
    "affected_services": ["service-a", "service-b"],
    "business_impact": "high|medium|low"
  },
  
  "code_review_results": {
    "overall_assessment": "APPROVE|REQUEST_CHANGES|COMMENT",
    "confidence_score": 0.95,
    "critical_issues": [],
    "high_severity_issues": [],
    "open_questions": [],
    "reviewer_recommendation": "string"
  },
  
  "static_analysis_results": {
    "security_vulnerabilities": {
      "critical": 0,
      "high": 0,
      "medium": 3,
      "low": 5
    },
    "code_quality_score": 8.5,
    "technical_debt_added": "2 hours"
  },
  
  "test_results": {
    "unit_tests": {"total": 450, "passed": 450, "failed": 0},
    "integration_tests": {"total": 125, "passed": 123, "failed": 2},
    "e2e_tests": {"total": 50, "passed": 50, "failed": 0},
    "performance_tests": {"status": "passed", "degradation": "+2%"},
    "security_tests": {"status": "passed"},
    "coverage": {"current": 86.1, "delta": +0.9, "threshold": 80},
    "failed_test_details": [...]
  },
  
  "deployment_context": {
    "target_environments": ["dev", "staging", "production"],
    "deployment_type": "standard|hotfix|emergency",
    "deployment_window": "2024-10-31T20:00:00Z to 2024-10-31T22:00:00Z",
    "business_hours": false,
    "expected_duration": "45 minutes"
  },
  
  "system_health": {
    "dev": {"status": "healthy", "error_rate": 0.1, "latency_p99": 150},
    "staging": {"status": "healthy", "error_rate": 0.2, "latency_p99": 180},
    "production": {"status": "healthy", "error_rate": 0.15, "latency_p99": 200},
    "recent_incidents": [],
    "ongoing_incidents": []
  },
  
  "dependencies": {
    "service_dependencies": ["auth-service", "payment-service"],
    "external_dependencies": ["stripe-api", "sendgrid"],
    "database_migrations": ["20241031_add_index.sql"],
    "infrastructure_changes": []
  },
  
  "historical_data": {
    "similar_deployments": {
      "success_rate": 0.94,
      "average_duration": "38 minutes",
      "common_issues": ["database connection timeout", "cache invalidation"]
    },
    "service_stability": {
      "recent_deploy_success_rate": 0.96,
      "mttr": "12 minutes",
      "mtbf": "15 days"
    }
  },
  
  "compliance_requirements": {
    "requires_approval": ["security-team", "compliance-team"],
    "audit_logging": true,
    "data_privacy": "GDPR compliant",
    "change_window": "must deploy off-peak hours"
  }
}
```

# Output Format
```json
{
  "decision": "
