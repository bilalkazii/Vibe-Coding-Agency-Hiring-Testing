{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import List, Dict, Any, Tuple, Optional, Union\n",
        "import pandas as pd\n",
        "\n",
        "# Component 1: Data Processor (returns dict with specific structure)\n",
        "class DataProcessor:\n",
        "    \"\"\"AI Component 1 - processes raw data and returns structured dict\"\"\"\n",
        "\n",
        "    def process_data(self, raw_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        \"\"\"Process raw data and return structured dict.\"\"\"\n",
        "        if not isinstance(raw_data, list):\n",
        "            raise ValueError(\"Expected list input\")\n",
        "\n",
        "        result = {\n",
        "            'total_items': len(raw_data),\n",
        "            'processed_items': [],\n",
        "            'metadata': {'processing_time': 0.1, 'timestamp': '2024-01-01T12:00:00Z'}\n",
        "        }\n",
        "\n",
        "        for item in raw_data:\n",
        "            if isinstance(item, dict) and 'value' in item:\n",
        "                result['processed_items'].append({\n",
        "                    'id': item.get('id', 'unknown'),\n",
        "                    'processed_value': item['value'] * 2,\n",
        "                    'original_value': item['value'],\n",
        "                    'status': 'processed'\n",
        "                })\n",
        "            else:\n",
        "                result['processed_items'].append({\n",
        "                    'id': 'error',\n",
        "                    'processed_value': 0,\n",
        "                    'original_value': None,\n",
        "                    'status': 'failed'\n",
        "                })\n",
        "\n",
        "        return result\n",
        "\n",
        "# Component 2: Analytics Engine (expects JSON string, returns tuple)\n",
        "class AnalyticsEngine:\n",
        "    \"\"\"AI Component 2 - performs analytics on data, expects JSON string input\"\"\"\n",
        "\n",
        "    def analyze(self, json_data_string: str) -> Tuple[Optional[str], Union[Dict[str, float], str]]:\n",
        "        \"\"\"Analyze data from JSON string, return (summary, metrics) tuple.\"\"\"\n",
        "        try:\n",
        "            data = json.loads(json_data_string)\n",
        "        except json.JSONDecodeError:\n",
        "            return None, \"Invalid JSON format\"\n",
        "\n",
        "        if not isinstance(data, dict) or 'processed_items' not in data:\n",
        "            return None, \"Missing processed_items in data structure\"\n",
        "\n",
        "        items = data['processed_items']\n",
        "        if not isinstance(items, list):\n",
        "            return None, \"processed_items must be a list\"\n",
        "\n",
        "        # Extract numeric values for analysis\n",
        "        values = []\n",
        "        failed_count = 0\n",
        "\n",
        "        for item in items:\n",
        "            if isinstance(item, dict) and item.get('status') == 'processed':\n",
        "                if 'processed_value' in item and isinstance(item['processed_value'], (int, float)):\n",
        "                    values.append(item['processed_value'])\n",
        "            else:\n",
        "                failed_count += 1\n",
        "\n",
        "        if not values:\n",
        "            return None, \"No valid numeric data found for analysis\"\n",
        "\n",
        "        summary = f\"Analyzed {len(items)} items ({len(values)} successful, {failed_count} failed)\"\n",
        "        metrics = {\n",
        "            'avg_value': sum(values) / len(values),\n",
        "            'max_value': max(values),\n",
        "            'min_value': min(values),\n",
        "            'total_value': sum(values),\n",
        "            'success_rate': len(values) / len(items) if items else 0.0\n",
        "        }\n",
        "\n",
        "        return summary, metrics\n",
        "\n",
        "# Component 3: Report Generator (expects list of tuples, returns formatted string)\n",
        "class ReportGenerator:\n",
        "    \"\"\"AI Component 3 - generates reports from analytics results\"\"\"\n",
        "\n",
        "    def generate_report(self, analytics_results_list: List[Tuple[Optional[str], Union[Dict, str]]]) -> str:\n",
        "        \"\"\"Generate report from list of (summary, metrics) tuples.\"\"\"\n",
        "        if not isinstance(analytics_results_list, list):\n",
        "            return \"Error: Expected list input for report generation\"\n",
        "\n",
        "        if not analytics_results_list:\n",
        "            return \"Error: No data provided for report generation\"\n",
        "\n",
        "        report_lines = [\n",
        "            \"=\" * 50,\n",
        "            \"           ANALYSIS REPORT\",\n",
        "            \"=\" * 50\n",
        "        ]\n",
        "\n",
        "        for i, result in enumerate(analytics_results_list):\n",
        "            if not isinstance(result, tuple) or len(result) != 2:\n",
        "                report_lines.append(f\"\\nSection {i+1}: Invalid data format - expected (summary, metrics) tuple\")\n",
        "                continue\n",
        "\n",
        "            summary, metrics = result\n",
        "\n",
        "            if summary is None:\n",
        "                report_lines.append(f\"\\nSection {i+1}: Analysis failed\")\n",
        "                report_lines.append(f\"  Error: {metrics}\")\n",
        "                continue\n",
        "\n",
        "            report_lines.append(f\"\\nSection {i+1}: {summary}\")\n",
        "\n",
        "            if isinstance(metrics, dict):\n",
        "                report_lines.append(\"  Metrics:\")\n",
        "                for key, value in metrics.items():\n",
        "                    if isinstance(value, float):\n",
        "                        report_lines.append(f\"    {key}: {value:.2f}\")\n",
        "                    else:\n",
        "                        report_lines.append(f\"    {key}: {value}\")\n",
        "            else:\n",
        "                report_lines.append(f\"  Metrics: {metrics}\")\n",
        "\n",
        "        report_lines.append(\"\\n\" + \"=\" * 50)\n",
        "        return \"\\n\".join(report_lines)\n",
        "\n",
        "# Integration Functions\n",
        "\n",
        "def dict_to_json_adapter(data_dict: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Convert dictionary to JSON string for AnalyticsEngine.\n",
        "\n",
        "    Args:\n",
        "        data_dict: Dictionary from DataProcessor\n",
        "\n",
        "    Returns:\n",
        "        JSON string suitable for AnalyticsEngine\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # ✅ Validate input type\n",
        "        if not isinstance(data_dict, dict):\n",
        "            raise ValueError(\"Input must be a dictionary\")\n",
        "\n",
        "        # ✅ Convert to JSON with proper error handling\n",
        "        json_string = json.dumps(data_dict, indent=2, default=str)\n",
        "        return json_string\n",
        "\n",
        "    except (TypeError, ValueError) as e:\n",
        "        print(f\"JSON conversion error: {e}\")\n",
        "        return json.dumps({\"error\": f\"JSON conversion failed: {str(e)}\"})\n",
        "\n",
        "def validate_and_clean_raw_data(raw_data: Any) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Validate and clean raw input data.\n",
        "\n",
        "    Args:\n",
        "        raw_data: Input data of any type\n",
        "\n",
        "    Returns:\n",
        "        Cleaned list of dictionaries\n",
        "    \"\"\"\n",
        "    # ✅ Handle None or empty input\n",
        "    if raw_data is None:\n",
        "        return []\n",
        "\n",
        "    # ✅ Convert to list if not already\n",
        "    if not isinstance(raw_data, list):\n",
        "        return []\n",
        "\n",
        "    cleaned_data = []\n",
        "\n",
        "    for item in raw_data:\n",
        "        # ✅ Handle different data types\n",
        "        if isinstance(item, dict):\n",
        "            # Ensure required structure\n",
        "            cleaned_item = {\n",
        "                'id': item.get('id', 'unknown'),\n",
        "                'value': item.get('value', 0)\n",
        "            }\n",
        "            cleaned_data.append(cleaned_item)\n",
        "        elif isinstance(item, (int, float)):\n",
        "            # Convert simple numbers to dict format\n",
        "            cleaned_data.append({'id': 'auto_generated', 'value': item})\n",
        "        # Skip other invalid types\n",
        "\n",
        "    return cleaned_data\n",
        "\n",
        "def integrated_pipeline(raw_data_list: List[Any]) -> str:\n",
        "    \"\"\"\n",
        "    Integrate all three components to process data end-to-end.\n",
        "\n",
        "    This function should:\n",
        "    1. Validate and clean each raw dataset\n",
        "    2. Process each dataset through DataProcessor\n",
        "    3. Convert results to format expected by AnalyticsEngine\n",
        "    4. Run analytics on each processed dataset\n",
        "    5. Collect all analytics results\n",
        "    6. Generate final report using ReportGenerator\n",
        "    7. Handle all errors gracefully\n",
        "\n",
        "    Args:\n",
        "        raw_data_list: List of raw data sets to process\n",
        "\n",
        "    Returns:\n",
        "        str: Final report combining all analyses\n",
        "    \"\"\"\n",
        "    # ✅ Initialize components\n",
        "    processor = DataProcessor()\n",
        "    analytics = AnalyticsEngine()\n",
        "    reporter = ReportGenerator()\n",
        "\n",
        "    analytics_results = []\n",
        "\n",
        "    # ✅ Process each dataset\n",
        "    for i, raw_data in enumerate(raw_data_list):\n",
        "        print(f\"Processing dataset {i+1}...\")\n",
        "\n",
        "        try:\n",
        "            # Step 1: Validate and clean data\n",
        "            cleaned_data = validate_and_clean_raw_data(raw_data)\n",
        "\n",
        "            if not cleaned_data:\n",
        "                print(f\"  Warning: Dataset {i+1} has no valid data after cleaning\")\n",
        "                analytics_results.append((None, \"No valid data to process\"))\n",
        "                continue\n",
        "\n",
        "            # Step 2: Process through DataProcessor\n",
        "            processed_data = processor.process_data(cleaned_data)\n",
        "\n",
        "            # Step 3: Convert to JSON for AnalyticsEngine\n",
        "            json_data = dict_to_json_adapter(processed_data)\n",
        "\n",
        "            # Step 4: Run analytics\n",
        "            analysis_result = analytics.analyze(json_data)\n",
        "\n",
        "            # Step 5: Collect results\n",
        "            analytics_results.append(analysis_result)\n",
        "\n",
        "            print(f\"  Successfully processed dataset {i+1}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # ✅ Handle any errors gracefully\n",
        "            error_msg = f\"Error processing dataset {i+1}: {str(e)}\"\n",
        "            print(f\"  {error_msg}\")\n",
        "            analytics_results.append((None, error_msg))\n",
        "\n",
        "    # Step 6: Generate final report\n",
        "    final_report = reporter.generate_report(analytics_results)\n",
        "\n",
        "    return final_report\n",
        "\n",
        "def create_sample_data() -> List[List[Dict[str, Any]]]:\n",
        "    \"\"\"Create sample test data for the pipeline.\"\"\"\n",
        "    return [\n",
        "        # Dataset 1: Normal data\n",
        "        [\n",
        "            {'id': 'A1', 'value': 10},\n",
        "            {'id': 'A2', 'value': 20},\n",
        "            {'id': 'A3', 'value': 15}\n",
        "        ],\n",
        "        # Dataset 2: Smaller dataset\n",
        "        [\n",
        "            {'id': 'B1', 'value': 5},\n",
        "            {'id': 'B2', 'value': 25}\n",
        "        ],\n",
        "        # Dataset 3: Mixed data with issues\n",
        "        [\n",
        "            {'id': 'C1', 'value': 30},\n",
        "            {'id': 'C2'},  # Missing value\n",
        "            {'value': 40},  # Missing id\n",
        "            {'id': 'C4', 'value': 'invalid'},  # Invalid value type\n",
        "        ],\n",
        "        # Dataset 4: Empty dataset\n",
        "        []\n",
        "    ]\n",
        "\n",
        "# Enhanced test with error cases\n",
        "def create_comprehensive_test_data():\n",
        "    \"\"\"Create comprehensive test data including edge cases.\"\"\"\n",
        "    return [\n",
        "        # Normal data\n",
        "        [{'id': 'X1', 'value': 100}, {'id': 'X2', 'value': 200}],\n",
        "        # Mixed valid/invalid\n",
        "        [{'id': 'Y1', 'value': 50}, {'invalid': 'data'}, 123],\n",
        "        # All invalid\n",
        "        [None, 'string', [], {}],\n",
        "        # Empty\n",
        "        []\n",
        "    ]\n",
        "\n",
        "# Test the integration\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Testing component integration...\")\n",
        "\n",
        "    # Test individual components first\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TESTING INDIVIDUAL COMPONENTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    processor = DataProcessor()\n",
        "    analytics = AnalyticsEngine()\n",
        "    reporter = ReportGenerator()\n",
        "\n",
        "    # Test DataProcessor\n",
        "    test_data = [{'id': 'test', 'value': 10}]\n",
        "    processed = processor.process_data(test_data)\n",
        "    print(f\"✓ DataProcessor output keys: {list(processed.keys())}\")\n",
        "\n",
        "    # Test AnalyticsEngine\n",
        "    json_data = dict_to_json_adapter(processed)\n",
        "    analysis_result = analytics.analyze(json_data)\n",
        "    print(f\"✓ AnalyticsEngine output: {analysis_result[0]}\")\n",
        "\n",
        "    # Test ReportGenerator\n",
        "    report = reporter.generate_report([analysis_result])\n",
        "    print(f\"✓ ReportGenerator output length: {len(report)} characters\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TESTING INTEGRATED PIPELINE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Test full pipeline\n",
        "    sample_datasets = create_sample_data()\n",
        "\n",
        "    try:\n",
        "        final_report = integrated_pipeline(sample_datasets)\n",
        "        print(\"🎉 Integration successful!\")\n",
        "        print(\"\\nFINAL REPORT:\")\n",
        "        print(final_report)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Integration failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TESTING EDGE CASES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Test edge cases\n",
        "    edge_cases = create_comprehensive_test_data()\n",
        "    edge_report = integrated_pipeline(edge_cases)\n",
        "    print(\"Edge cases report generated successfully!\")\n",
        "    print(f\"Report preview: {edge_report[:200]}...\")\n",
        "\n",
        "# Additional utility functions\n",
        "def pipeline_with_metrics(raw_data_list: List[Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Enhanced pipeline that returns metrics along with report.\n",
        "\n",
        "    Returns:\n",
        "        Dict containing report, success_rate, and processing_stats\n",
        "    \"\"\"\n",
        "    processor = DataProcessor()\n",
        "    analytics = AnalyticsEngine()\n",
        "    reporter = ReportGenerator()\n",
        "\n",
        "    analytics_results = []\n",
        "    processing_stats = {\n",
        "        'total_datasets': len(raw_data_list),\n",
        "        'successful_processing': 0,\n",
        "        'failed_processing': 0,\n",
        "        'total_items_processed': 0\n",
        "    }\n",
        "\n",
        "    for i, raw_data in enumerate(raw_data_list):\n",
        "        try:\n",
        "            cleaned_data = validate_and_clean_raw_data(raw_data)\n",
        "            processing_stats['total_items_processed'] += len(cleaned_data)\n",
        "\n",
        "            if cleaned_data:\n",
        "                processed_data = processor.process_data(cleaned_data)\n",
        "                json_data = dict_to_json_adapter(processed_data)\n",
        "                analysis_result = analytics.analyze(json_data)\n",
        "\n",
        "                if analysis_result[0] is not None:  # Success case\n",
        "                    processing_stats['successful_processing'] += 1\n",
        "                else:\n",
        "                    processing_stats['failed_processing'] += 1\n",
        "\n",
        "                analytics_results.append(analysis_result)\n",
        "            else:\n",
        "                processing_stats['failed_processing'] += 1\n",
        "                analytics_results.append((None, \"No valid data\"))\n",
        "        except Exception as e:\n",
        "            processing_stats['failed_processing'] += 1\n",
        "            analytics_results.append((None, f\"Processing error: {str(e)}\"))\n",
        "\n",
        "    final_report = reporter.generate_report(analytics_results)\n",
        "\n",
        "    return {\n",
        "        'report': final_report,\n",
        "        'processing_stats': processing_stats,\n",
        "        'success_rate': processing_stats['successful_processing'] / processing_stats['total_datasets'] if processing_stats['total_datasets'] > 0 else 0\n",
        "    }\n",
        "# Test Output:\n",
        "# Quick test\n",
        "sample_data = create_sample_data()\n",
        "result = integrated_pipeline(sample_data)\n",
        "print(result)\n",
        "\n",
        "# Test with metrics\n",
        "metrics_result = pipeline_with_metrics(sample_data)\n",
        "print(f\"\\nSuccess Rate: {metrics_result['success_rate']:.2%}\")\n",
        "print(f\"Processing Stats: {metrics_result['processing_stats']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak-eDC-eaT0S",
        "outputId": "ee4bb132-6fcf-4a49-e1b0-bc6b81a8d96a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing component integration...\n",
            "\n",
            "============================================================\n",
            "TESTING INDIVIDUAL COMPONENTS\n",
            "============================================================\n",
            "✓ DataProcessor output keys: ['total_items', 'processed_items', 'metadata']\n",
            "✓ AnalyticsEngine output: Analyzed 1 items (1 successful, 0 failed)\n",
            "✓ ReportGenerator output length: 345 characters\n",
            "\n",
            "============================================================\n",
            "TESTING INTEGRATED PIPELINE\n",
            "============================================================\n",
            "Processing dataset 1...\n",
            "  Successfully processed dataset 1\n",
            "Processing dataset 2...\n",
            "  Successfully processed dataset 2\n",
            "Processing dataset 3...\n",
            "  Successfully processed dataset 3\n",
            "Processing dataset 4...\n",
            "  Warning: Dataset 4 has no valid data after cleaning\n",
            "🎉 Integration successful!\n",
            "\n",
            "FINAL REPORT:\n",
            "==================================================\n",
            "           ANALYSIS REPORT\n",
            "==================================================\n",
            "\n",
            "Section 1: Analyzed 3 items (3 successful, 0 failed)\n",
            "  Metrics:\n",
            "    avg_value: 30.00\n",
            "    max_value: 40\n",
            "    min_value: 20\n",
            "    total_value: 90\n",
            "    success_rate: 1.00\n",
            "\n",
            "Section 2: Analyzed 2 items (2 successful, 0 failed)\n",
            "  Metrics:\n",
            "    avg_value: 30.00\n",
            "    max_value: 50\n",
            "    min_value: 10\n",
            "    total_value: 60\n",
            "    success_rate: 1.00\n",
            "\n",
            "Section 3: Analyzed 4 items (3 successful, 0 failed)\n",
            "  Metrics:\n",
            "    avg_value: 46.67\n",
            "    max_value: 80\n",
            "    min_value: 0\n",
            "    total_value: 140\n",
            "    success_rate: 0.75\n",
            "\n",
            "Section 4: Analysis failed\n",
            "  Error: No valid data to process\n",
            "\n",
            "==================================================\n",
            "\n",
            "============================================================\n",
            "TESTING EDGE CASES\n",
            "============================================================\n",
            "Processing dataset 1...\n",
            "  Successfully processed dataset 1\n",
            "Processing dataset 2...\n",
            "  Successfully processed dataset 2\n",
            "Processing dataset 3...\n",
            "  Successfully processed dataset 3\n",
            "Processing dataset 4...\n",
            "  Warning: Dataset 4 has no valid data after cleaning\n",
            "Edge cases report generated successfully!\n",
            "Report preview: ==================================================\n",
            "           ANALYSIS REPORT\n",
            "==================================================\n",
            "\n",
            "Section 1: Analyzed 2 items (2 successful, 0 failed)\n",
            "  Metrics:\n",
            "    av...\n",
            "Processing dataset 1...\n",
            "  Successfully processed dataset 1\n",
            "Processing dataset 2...\n",
            "  Successfully processed dataset 2\n",
            "Processing dataset 3...\n",
            "  Successfully processed dataset 3\n",
            "Processing dataset 4...\n",
            "  Warning: Dataset 4 has no valid data after cleaning\n",
            "==================================================\n",
            "           ANALYSIS REPORT\n",
            "==================================================\n",
            "\n",
            "Section 1: Analyzed 3 items (3 successful, 0 failed)\n",
            "  Metrics:\n",
            "    avg_value: 30.00\n",
            "    max_value: 40\n",
            "    min_value: 20\n",
            "    total_value: 90\n",
            "    success_rate: 1.00\n",
            "\n",
            "Section 2: Analyzed 2 items (2 successful, 0 failed)\n",
            "  Metrics:\n",
            "    avg_value: 30.00\n",
            "    max_value: 50\n",
            "    min_value: 10\n",
            "    total_value: 60\n",
            "    success_rate: 1.00\n",
            "\n",
            "Section 3: Analyzed 4 items (3 successful, 0 failed)\n",
            "  Metrics:\n",
            "    avg_value: 46.67\n",
            "    max_value: 80\n",
            "    min_value: 0\n",
            "    total_value: 140\n",
            "    success_rate: 0.75\n",
            "\n",
            "Section 4: Analysis failed\n",
            "  Error: No valid data to process\n",
            "\n",
            "==================================================\n",
            "\n",
            "Success Rate: 75.00%\n",
            "Processing Stats: {'total_datasets': 4, 'successful_processing': 3, 'failed_processing': 1, 'total_items_processed': 9}\n"
          ]
        }
      ]
    }
  ]
}